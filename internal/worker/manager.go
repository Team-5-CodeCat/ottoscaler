// Package worker provides Worker Pod lifecycle management for Ottoscaler.
//
// ì´ íŒ¨í‚¤ì§€ëŠ” Otto agent Worker Podë“¤ì˜ ì „ì²´ ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
// Kubernetes APIë¥¼ í†µí•´ Worker Podë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±, ëª¨ë‹ˆí„°ë§, ì •ë¦¬í•˜ëŠ”
// ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
//
// ì£¼ìš” ê¸°ëŠ¥:
//   - ë™ì‹œ ë‹¤ì¤‘ Worker Pod ìƒì„± ë° ê´€ë¦¬ (RunMultipleWorkers)
//   - Pod ìƒíƒœ ëª¨ë‹ˆí„°ë§ (2ì´ˆ ê°„ê²© í´ë§)
//   - ì‘ì—… ì™„ë£Œ í›„ ìë™ ì •ë¦¬ (CleanupPod)
//   - ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„ ë¡œì§
//   - ë°°ì¹˜ ë‹¨ìœ„ Worker ê´€ë¦¬
//   - í™œì„± Pod ëª©ë¡ ì¡°íšŒ (scale_down ì¤€ë¹„)
//
// ì‚¬ìš© ì˜ˆì‹œ:
//
//	k8sClient, _ := k8s.NewClient("default")
//	manager := worker.NewManager(k8sClient, "default")
//
//	config := worker.WorkerConfig{
//		Name:    "otto-agent-1",
//		Image:   "busybox:latest",
//		Command: []string{"sh", "-c"},
//		Args:    []string{"echo hello"},
//		Labels:  map[string]string{"app": "otto-agent"},
//	}
//
//	err := manager.CreateAndWaitForWorker(ctx, config)
//
// Worker ManagerëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŒ¨í„´ìœ¼ë¡œ Podë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤:
//  1. ìƒì„± (CreateWorkerPod)
//  2. ëª¨ë‹ˆí„°ë§ (WaitForPodCompletion)
//  3. ì •ë¦¬ (CleanupPod)
package worker

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	"github.com/Team-5-CodeCat/ottoscaler/internal/k8s"
)

const (
	// PodMonitoringIntervalì€ Pod ìƒíƒœ í™•ì¸ ê°„ê²©ì…ë‹ˆë‹¤
	PodMonitoringInterval = 2 * time.Second
	// DefaultWorkerImageëŠ” ê¸°ë³¸ Worker ì´ë¯¸ì§€ì…ë‹ˆë‹¤
	DefaultWorkerImage = "busybox:latest"
	// WorkerContainerNameì€ Worker ì»¨í…Œì´ë„ˆ ì´ë¦„ì…ë‹ˆë‹¤
	WorkerContainerName = "worker"
)

// Manager manages the lifecycle of Otto agent Worker Pods.
//
// ManagerëŠ” Worker Podë“¤ì˜ ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ë¦¬í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ì…ë‹ˆë‹¤.
//
// ì±…ì„:
//   - Worker Pod ìƒì„± ë° ì„¤ì •
//   - ë™ì‹œ ì‹¤í–‰ë˜ëŠ” ë‹¤ì¤‘ Worker ê´€ë¦¬
//   - Pod ì™„ë£Œ ìƒíƒœ ëª¨ë‹ˆí„°ë§
//   - ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘ ë° ìŠ¤íŠ¸ë¦¬ë°
//   - ìë™ ì •ë¦¬ ë° ë¦¬ì†ŒìŠ¤ í•´ì œ
type Manager struct {
	k8sClient    *k8s.Client
	namespace    string
	logCollector *LogCollector
}

// WorkerConfig contains configuration for creating a Worker Pod.
//
// WorkerConfigëŠ” Worker Pod ìƒì„±ì— í•„ìš”í•œ ì„¤ì •ì„ ë‹´ëŠ” êµ¬ì¡°ì²´ì…ë‹ˆë‹¤.
//
// í•„ë“œ ì„¤ëª…:
//   - Name: Pod ì´ë¦„ (Kubernetes ë„¤ì´ë° ê·œì¹™ ì¤€ìˆ˜)
//   - Image: ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
//   - Command: ì‹¤í–‰í•  ëª…ë ¹ì–´
//   - Args: ëª…ë ¹ì–´ ì¸ì
//   - Labels: Podì— ì ìš©í•  ë¼ë²¨ (ê´€ë¦¬ ë° ì‹ë³„ìš©)
//   - Resources: CPU/ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ ì œí•œ (ì„ íƒì )
type WorkerConfig struct {
	Name      string            `json:"name"`      // Pod ì´ë¦„
	Image     string            `json:"image"`     // ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
	Command   []string          `json:"command"`   // ì‹¤í–‰ ëª…ë ¹ì–´
	Args      []string          `json:"args"`      // ëª…ë ¹ì–´ ì¸ì
	Labels    map[string]string `json:"labels"`    // Pod ë¼ë²¨
	Resources *ResourceConfig   `json:"resources"` // ë¦¬ì†ŒìŠ¤ ì„¤ì • (ì„ íƒì )
}

// ResourceConfig defines resource limits for Worker Pods.
//
// ResourceConfigëŠ” Worker Podì˜ ë¦¬ì†ŒìŠ¤ ì œí•œì„ ì •ì˜í•©ë‹ˆë‹¤.
type ResourceConfig struct {
	CPURequest    string `json:"cpu_request"`    // CPU ìš”ì²­ëŸ‰ (ì˜ˆ: "100m")
	MemoryRequest string `json:"memory_request"` // ë©”ëª¨ë¦¬ ìš”ì²­ëŸ‰ (ì˜ˆ: "128Mi")
	CPULimit      string `json:"cpu_limit"`      // CPU ì œí•œ (ì˜ˆ: "500m")
	MemoryLimit   string `json:"memory_limit"`   // ë©”ëª¨ë¦¬ ì œí•œ (ì˜ˆ: "256Mi")
}

// WorkerStatus represents the execution result of a Worker Pod.
//
// WorkerStatusëŠ” Worker Podì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
type WorkerStatus struct {
	Name      string        `json:"name"`       // Pod ì´ë¦„
	Status    string        `json:"status"`     // ìµœì¢… ìƒíƒœ
	StartTime time.Time     `json:"start_time"` // ì‹œì‘ ì‹œê°„
	EndTime   time.Time     `json:"end_time"`   // ì¢…ë£Œ ì‹œê°„
	Duration  time.Duration `json:"duration"`   // ì‹¤í–‰ ì‹œê°„
	Error     error         `json:"error"`      // ì—ëŸ¬ (ì‹¤íŒ¨ ì‹œ)
}

// LogCollector manages log streaming for worker pods
//
// LogCollectorëŠ” Worker Podë“¤ì˜ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
type LogCollector struct {
	k8sClient  *k8s.Client
	activeLogs map[string]context.CancelFunc // Podë³„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° ì·¨ì†Œ í•¨ìˆ˜
	logMutex   sync.RWMutex

	// Log forwarding configuration
	enableForwarding bool
	logBufferSize    int
}

// NewLogCollector creates a new log collector
//
// NewLogCollectorëŠ” ìƒˆë¡œìš´ ë¡œê·¸ ìˆ˜ì§‘ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
func NewLogCollector(k8sClient *k8s.Client) *LogCollector {
	return &LogCollector{
		k8sClient:        k8sClient,
		activeLogs:       make(map[string]context.CancelFunc),
		enableForwarding: true,
		logBufferSize:    1000,
	}
}

// NewManagerëŠ” ìƒˆë¡œìš´ Worker Managerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
//
// Parameters:
//   - k8sClient: Kubernetes API í´ë¼ì´ì–¸íŠ¸
//   - namespace: Worker Podë¥¼ ìƒì„±í•  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
//
// Returns:
//   - *Manager: ì´ˆê¸°í™”ëœ ë§¤ë‹ˆì €
func NewManager(k8sClient *k8s.Client, namespace string) *Manager {
	if namespace == "" {
		namespace = "default"
	}

	log.Printf("ğŸ‘· Worker Manager initialized for namespace: %s", namespace)

	logCollector := NewLogCollector(k8sClient)

	return &Manager{
		k8sClient:    k8sClient,
		namespace:    namespace,
		logCollector: logCollector,
	}
}

// CreateWorkerPodëŠ” WorkerConfigë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ Worker Podë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
//
// ìƒì„±ë˜ëŠ” Podì˜ íŠ¹ì§•:
//   - RestartPolicy: Never (ì¼íšŒì„± ì‘ì—…)
//   - ê´€ë¦¬ ë¼ë²¨ ìë™ ì¶”ê°€
//   - ë¦¬ì†ŒìŠ¤ ì œí•œ ì ìš© (ì„¤ì •ëœ ê²½ìš°)
func (m *Manager) CreateWorkerPod(ctx context.Context, config WorkerConfig) (*v1.Pod, error) {
	// Pod ìŠ¤í™ ìƒì„±
	podSpec := m.buildPodSpec(config)

	log.Printf("ğŸš€ Creating worker pod: %s (image: %s)", config.Name, config.Image)

	createdPod, err := m.k8sClient.CreatePod(ctx, podSpec)
	if err != nil {
		return nil, fmt.Errorf("failed to create worker pod %s: %w", config.Name, err)
	}

	return createdPod, nil
}

// buildPodSpecì€ WorkerConfigë¡œë¶€í„° Pod ìŠ¤í™ì„ êµ¬ì„±í•©ë‹ˆë‹¤
func (m *Manager) buildPodSpec(config WorkerConfig) *v1.Pod {
	// ê¸°ë³¸ ë¼ë²¨ ì„¤ì •
	labels := make(map[string]string)
	for k, v := range config.Labels {
		labels[k] = v
	}

	// í•„ìˆ˜ ê´€ë¦¬ ë¼ë²¨ ì¶”ê°€
	if labels["managed-by"] == "" {
		labels["managed-by"] = "ottoscaler"
	}
	if labels["app"] == "" {
		labels["app"] = "otto-agent"
	}

	// ì»¨í…Œì´ë„ˆ ìŠ¤í™ ìƒì„±
	container := v1.Container{
		Name:    WorkerContainerName,
		Image:   config.Image,
		Command: config.Command,
		Args:    config.Args,
	}

	// ë¦¬ì†ŒìŠ¤ ì œí•œ ì ìš©
	if config.Resources != nil {
		container.Resources = m.buildResourceRequirements(config.Resources)
	}

	return &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:      config.Name,
			Namespace: m.namespace,
			Labels:    labels,
			Annotations: map[string]string{
				"ottoscaler.io/created-at": time.Now().Format(time.RFC3339),
			},
		},
		Spec: v1.PodSpec{
			RestartPolicy: v1.RestartPolicyNever,
			Containers:    []v1.Container{container},
		},
	}
}

// buildResourceRequirementsëŠ” ResourceConfigë¥¼ Kubernetes ResourceRequirementsë¡œ ë³€í™˜í•©ë‹ˆë‹¤
func (m *Manager) buildResourceRequirements(config *ResourceConfig) v1.ResourceRequirements {
	// TODO: ì‹¤ì œ ë¦¬ì†ŒìŠ¤ íŒŒì‹± êµ¬í˜„
	// í˜„ì¬ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜
	return v1.ResourceRequirements{}
}

// WaitForPodCompletionì€ Podê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.
//
// ëª¨ë‹ˆí„°ë§ ë°©ì‹:
//   - 2ì´ˆ ê°„ê²©ìœ¼ë¡œ Pod ìƒíƒœ í´ë§
//   - Succeeded: ì •ìƒ ì™„ë£Œ
//   - Failed: ì‹¤íŒ¨
//   - Running/Pending: ê³„ì† ëŒ€ê¸°
//
// Context ì·¨ì†Œ ì‹œ ì¦‰ì‹œ ë°˜í™˜í•©ë‹ˆë‹¤.
func (m *Manager) WaitForPodCompletion(ctx context.Context, podName string) error {
	log.Printf("â³ Waiting for pod %s to complete...", podName)

	ticker := time.NewTicker(PodMonitoringInterval)
	defer ticker.Stop()

	startTime := time.Now()

	for {
		select {
		case <-ctx.Done():
			log.Printf("â° Pod monitoring cancelled for %s after %v", podName, time.Since(startTime))
			return ctx.Err()

		case <-ticker.C:
			pod, err := m.k8sClient.GetPod(ctx, podName)
			if err != nil {
				log.Printf("âš ï¸ Error getting pod %s: %v", podName, err)
				continue
			}

			// ìƒíƒœë³„ ì²˜ë¦¬
			switch pod.Status.Phase {
			case v1.PodSucceeded:
				duration := time.Since(startTime)
				log.Printf("âœ… Pod %s completed successfully in %v", podName, duration)
				return nil

			case v1.PodFailed:
				duration := time.Since(startTime)
				reason := m.getPodFailureReason(pod)
				log.Printf("âŒ Pod %s failed after %v: %s", podName, duration, reason)
				return fmt.Errorf("pod %s failed: %s", podName, reason)

			case v1.PodRunning:
				log.Printf("ğŸƒ Pod %s is running... (elapsed: %v)", podName, time.Since(startTime))

			case v1.PodPending:
				log.Printf("â¸ï¸ Pod %s is pending... (elapsed: %v)", podName, time.Since(startTime))

			default:
				log.Printf("â“ Pod %s in unknown state: %s", podName, pod.Status.Phase)
			}
		}
	}
}

// getPodFailureReasonì€ Pod ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤
func (m *Manager) getPodFailureReason(pod *v1.Pod) string {
	// ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
	for _, containerStatus := range pod.Status.ContainerStatuses {
		if containerStatus.State.Terminated != nil {
			terminated := containerStatus.State.Terminated
			if terminated.ExitCode != 0 {
				return fmt.Sprintf("container exited with code %d: %s",
					terminated.ExitCode, terminated.Reason)
			}
		}

		if containerStatus.State.Waiting != nil {
			waiting := containerStatus.State.Waiting
			return fmt.Sprintf("container waiting: %s - %s",
				waiting.Reason, waiting.Message)
		}
	}

	// Pod ì¡°ê±´ í™•ì¸
	for _, condition := range pod.Status.Conditions {
		if condition.Type == v1.PodReady && condition.Status == v1.ConditionFalse {
			return fmt.Sprintf("pod not ready: %s", condition.Message)
		}
	}

	return "unknown failure reason"
}

// CleanupPodëŠ” ì™„ë£Œëœ Podë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤
func (m *Manager) CleanupPod(ctx context.Context, podName string) error {
	log.Printf("ğŸ§¹ Cleaning up pod: %s", podName)

	if err := m.k8sClient.DeletePod(ctx, podName); err != nil {
		return fmt.Errorf("failed to cleanup pod %s: %w", podName, err)
	}

	return nil
}

// CreateAndWaitForWorkerëŠ” Worker Podë¥¼ ìƒì„±í•˜ê³  ì™„ë£Œê¹Œì§€ ëŒ€ê¸°í•œ í›„ ì •ë¦¬í•©ë‹ˆë‹¤.
//
// ì „ì²´ í”„ë¡œì„¸ìŠ¤:
//  1. Pod ìƒì„±
//  2. ë¡œê·¸ ìˆ˜ì§‘ ì‹œì‘
//  3. ì™„ë£Œ ëŒ€ê¸° (ëª¨ë‹ˆí„°ë§)
//  4. ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨
//  5. ìë™ ì •ë¦¬
//
// ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì •ë¦¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
func (m *Manager) CreateAndWaitForWorker(ctx context.Context, config WorkerConfig) error {
	startTime := time.Now()

	// 1. Worker Pod ìƒì„±
	_, err := m.CreateWorkerPod(ctx, config)
	if err != nil {
		return fmt.Errorf("worker creation failed: %w", err)
	}

	// 2. ë¡œê·¸ ìˆ˜ì§‘ ì‹œì‘ (taskID ì¶”ì¶œ)
	taskID := config.Labels["task-id"]
	if taskID == "" {
		taskID = "unknown"
	}

	if err := m.logCollector.StartLogCollection(ctx, config.Name, taskID); err != nil {
		log.Printf("âš ï¸ Warning: failed to start log collection for %s: %v", config.Name, err)
	}

	// 3. ì™„ë£Œ ëŒ€ê¸°
	err = m.WaitForPodCompletion(ctx, config.Name)

	// 4. ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨
	m.logCollector.StopLogCollection(config.Name)

	// 5. ì •ë¦¬ (ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ ìˆ˜í–‰)
	cleanupCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()

	if cleanupErr := m.CleanupPod(cleanupCtx, config.Name); cleanupErr != nil {
		log.Printf("âš ï¸ Warning: failed to cleanup pod %s: %v", config.Name, cleanupErr)
	}

	totalDuration := time.Since(startTime)

	if err != nil {
		log.Printf("âŒ Worker %s failed after %v: %v", config.Name, totalDuration, err)
		return fmt.Errorf("worker %s failed: %w", config.Name, err)
	}

	log.Printf("âœ… Worker %s completed successfully in %v", config.Name, totalDuration)
	return nil
}

// RunMultipleWorkersëŠ” ì—¬ëŸ¬ Worker Podë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ëª¨ë“  ì™„ë£Œë¥¼ ëŒ€ê¸°í•©ë‹ˆë‹¤.
//
// íŠ¹ì§•:
//   - ëª¨ë“  Workerë¥¼ ë™ì‹œì— ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)
//   - ê° WorkerëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ (í•˜ë‚˜ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ Worker ê³„ì† ì‹¤í–‰)
//   - ëª¨ë“  Worker ì™„ë£Œ í›„ ì „ì²´ ê²°ê³¼ ë°˜í™˜
//   - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ìƒì„¸í•œ ì—ëŸ¬ ì •ë³´ ì œê³µ
func (m *Manager) RunMultipleWorkers(ctx context.Context, configs []WorkerConfig) error {
	if len(configs) == 0 {
		return fmt.Errorf("no worker configurations provided")
	}

	log.Printf("ğŸš€ Starting batch of %d worker pods", len(configs))
	startTime := time.Now()

	// ê²°ê³¼ ìˆ˜ì§‘ìš© ì±„ë„
	results := make(chan WorkerStatus, len(configs))

	// ëª¨ë“  Workerë¥¼ ë™ì‹œì— ì‹œì‘
	for _, config := range configs {
		go func(cfg WorkerConfig) {
			workerStartTime := time.Now()
			status := WorkerStatus{
				Name:      cfg.Name,
				StartTime: workerStartTime,
			}

			// Worker ì‹¤í–‰
			err := m.CreateAndWaitForWorker(ctx, cfg)

			// ê²°ê³¼ ê¸°ë¡
			status.EndTime = time.Now()
			status.Duration = status.EndTime.Sub(status.StartTime)
			status.Error = err

			if err != nil {
				status.Status = "failed"
			} else {
				status.Status = "succeeded"
			}

			results <- status
		}(config)
	}

	// ëª¨ë“  Worker ì™„ë£Œ ëŒ€ê¸° ë° ê²°ê³¼ ìˆ˜ì§‘
	var (
		successCount = 0
		failureCount = 0
		errors       []error
		statuses     = make([]WorkerStatus, 0, len(configs))
	)

	for i := 0; i < len(configs); i++ {
		status := <-results
		statuses = append(statuses, status)

		if status.Error != nil {
			failureCount++
			errors = append(errors, fmt.Errorf("worker %s: %w", status.Name, status.Error))
		} else {
			successCount++
		}
	}

	totalDuration := time.Since(startTime)

	// ê²°ê³¼ ë¡œê¹…
	log.Printf("ğŸ“Š Batch completed in %v: %d succeeded, %d failed",
		totalDuration, successCount, failureCount)

	// ê°œë³„ Worker ìƒíƒœ ë¡œê¹…
	for _, status := range statuses {
		if status.Error != nil {
			log.Printf("  âŒ %s: failed in %v", status.Name, status.Duration)
		} else {
			log.Printf("  âœ… %s: succeeded in %v", status.Name, status.Duration)
		}
	}

	// ì‹¤íŒ¨ê°€ ìˆëŠ” ê²½ìš° ì—ëŸ¬ ë°˜í™˜
	if failureCount > 0 {
		return fmt.Errorf("batch partially failed: %d/%d workers failed: %v",
			failureCount, len(configs), errors)
	}

	log.Printf("ğŸ‰ All %d workers completed successfully!", successCount)
	return nil
}

// ListActivePodsëŠ” í˜„ì¬ í™œì„± ìƒíƒœì¸ Worker Pod ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
//
// í™œì„± ìƒíƒœ ì •ì˜:
//   - Pending: ì‹œì‘ ëŒ€ê¸° ì¤‘
//   - Running: ì‹¤í–‰ ì¤‘
//
// ì´ ë©”ì„œë“œëŠ” scale_down ê¸°ëŠ¥ êµ¬í˜„ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.
func (m *Manager) ListActivePods(ctx context.Context) ([]*v1.Pod, error) {
	// managed-by=ottoscaler ë¼ë²¨ë¡œ í•„í„°ë§
	podList, err := m.k8sClient.ListPods(ctx, "managed-by=ottoscaler")
	if err != nil {
		return nil, fmt.Errorf("failed to list worker pods: %w", err)
	}

	activePods := make([]*v1.Pod, 0)

	for i := range podList.Items {
		pod := &podList.Items[i]

		// í™œì„± ìƒíƒœì¸ Podë§Œ í¬í•¨
		if pod.Status.Phase == v1.PodPending || pod.Status.Phase == v1.PodRunning {
			activePods = append(activePods, pod)
		}
	}

	log.Printf("ğŸ“‹ Found %d active worker pods", len(activePods))
	return activePods, nil
}

// TerminatePodsëŠ” ì§€ì •ëœ ìˆ˜ë§Œí¼ Worker Podë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
//
// ì¢…ë£Œ ì „ëµ:
//   - ê°€ì¥ ì˜¤ë˜ëœ Podë¶€í„° ì¢…ë£Œ (FIFO)
//   - Graceful termination ì‹œë„
//   - ê°•ì œ ì¢…ë£ŒëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
//
// TODO: scale_down ê¸°ëŠ¥ê³¼ í•¨ê»˜ êµ¬í˜„ ì˜ˆì •
func (m *Manager) TerminatePods(ctx context.Context, count int) error {
	// TODO: scale_down ê¸°ëŠ¥ êµ¬í˜„ ì‹œ ì¶”ê°€
	return fmt.Errorf("pod termination not implemented yet")
}

// StartLogCollection starts log collection for a worker pod
//
// StartLogCollectionì€ Worker Podì˜ ë¡œê·¸ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.
func (lc *LogCollector) StartLogCollection(ctx context.Context, podName string, taskID string) error {
	lc.logMutex.Lock()
	defer lc.logMutex.Unlock()

	// Check if already collecting logs for this pod
	if _, exists := lc.activeLogs[podName]; exists {
		log.Printf("ğŸ“œ Log collection already active for pod: %s", podName)
		return nil
	}

	// Create cancellation context for this pod's log collection
	logCtx, cancel := context.WithCancel(ctx)
	lc.activeLogs[podName] = cancel

	go func() {
		defer func() {
			lc.logMutex.Lock()
			delete(lc.activeLogs, podName)
			lc.logMutex.Unlock()
		}()

		// Wait a moment for pod to be ready for log collection
		select {
		case <-time.After(5 * time.Second):
		case <-logCtx.Done():
			return
		}

		// Start log streaming
		options := k8s.LogStreamOptions{
			Follow:     true,
			Timestamps: true,
			Container:  WorkerContainerName,
		}

		logChan, errChan := lc.k8sClient.StreamPodLogs(logCtx, podName, options)
		log.Printf("ğŸ“œ Started log collection for pod: %s", podName)

		for {
			select {
			case logEntry, ok := <-logChan:
				if !ok {
					log.Printf("ğŸ“œ Log collection completed for pod: %s", podName)
					return
				}

				// Process log entry
				lc.processLogEntry(logEntry, taskID)

			case err, ok := <-errChan:
				if !ok {
					return
				}
				if err != nil {
					log.Printf("âš ï¸ Log collection error for pod %s: %v", podName, err)
				}

			case <-logCtx.Done():
				log.Printf("ğŸ“œ Log collection cancelled for pod: %s", podName)
				return
			}
		}
	}()

	return nil
}

// StopLogCollection stops log collection for a worker pod
//
// StopLogCollectionì€ Worker Podì˜ ë¡œê·¸ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
func (lc *LogCollector) StopLogCollection(podName string) {
	lc.logMutex.Lock()
	defer lc.logMutex.Unlock()

	if cancel, exists := lc.activeLogs[podName]; exists {
		cancel()
		delete(lc.activeLogs, podName)
		log.Printf("ğŸ”Œ Pod %sì˜ ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨", podName)
	}
}

// processLogEntry processes a single log entry from a worker pod
//
// processLogEntryëŠ” Worker Podì—ì„œ ìˆ˜ì§‘ëœ ë¡œê·¸ ì—”íŠ¸ë¦¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
func (lc *LogCollector) processLogEntry(entry k8s.LogEntry, taskID string) {
	if !lc.enableForwarding {
		return
	}

	// Format log entry for display
	logLevel := "INFO"
	if entry.Source == "stderr" {
		logLevel = "ERROR"
	}

	// TODO: Forward to LogStreamingService for gRPC streaming to otto-handler
	// For now, just log locally with enhanced formatting
	log.Printf("ğŸ“œ [%s|%s] %s: %s",
		entry.PodName,
		taskID,
		logLevel,
		entry.Message)
}

// GetActiveLogCollections returns the list of pods currently being logged
//
// GetActiveLogCollectionsëŠ” í˜„ì¬ ë¡œê·¸ê°€ ìˆ˜ì§‘ë˜ê³  ìˆëŠ” Pod ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
func (lc *LogCollector) GetActiveLogCollections() []string {
	lc.logMutex.RLock()
	defer lc.logMutex.RUnlock()

	pods := make([]string, 0, len(lc.activeLogs))
	for podName := range lc.activeLogs {
		pods = append(pods, podName)
	}
	return pods
}

// StopAllLogCollections stops all active log collections
//
// StopAllLogCollectionsëŠ” ëª¨ë“  í™œì„± ë¡œê·¸ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
func (lc *LogCollector) StopAllLogCollections() {
	lc.logMutex.Lock()
	defer lc.logMutex.Unlock()

	for podName, cancel := range lc.activeLogs {
		cancel()
		log.Printf("ğŸ”Œ Pod %sì˜ ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨", podName)
	}

	// Clear the map
	lc.activeLogs = make(map[string]context.CancelFunc)
	log.Printf("ğŸ”Œ ëª¨ë“  ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨ë¨")
}
