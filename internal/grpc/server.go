// Package grpc provides gRPC server implementation for Ottoscaler.
//
// This package implements the OttoscalerService defined in the proto file,
// handling scale up/down requests from otto-handler and managing worker pod lifecycle.
package grpc

import (
	"context"
	"fmt"
	"log"
	"net"
	"sync"
	"time"

	"google.golang.org/grpc"
	"google.golang.org/grpc/codes"
	"google.golang.org/grpc/status"

	"github.com/Team-5-CodeCat/ottoscaler/internal/config"
	"github.com/Team-5-CodeCat/ottoscaler/internal/k8s"
	"github.com/Team-5-CodeCat/ottoscaler/internal/pipeline"
	"github.com/Team-5-CodeCat/ottoscaler/internal/worker"
	pb "github.com/Team-5-CodeCat/ottoscaler/pkg/proto/v1"
)

// Server implements the OttoscalerService gRPC server.
//
// ServerëŠ” Otto-handlerë¡œë¶€í„° ìŠ¤ì¼€ì¼ë§ ìš”ì²­ì„ ë°›ì•„ ì²˜ë¦¬í•˜ëŠ” gRPC ì„œë²„ì…ë‹ˆë‹¤.
// ëª¨ë“  gRPC ë©”ì„œë“œëŠ” contextë¥¼ ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì•„ ì·¨ì†Œì™€ íƒ€ì„ì•„ì›ƒì„ ì§€ì›í•©ë‹ˆë‹¤.
type Server struct {
	pb.UnimplementedOttoscalerServiceServer

	config          *config.Config
	workerManager   *worker.Manager
	k8sClient       *k8s.Client
	logStreamServer *LogStreamingServer
	
	// Pipeline ì‹¤í–‰ ê´€ë¦¬
	pipelineExecutors map[string]*pipeline.Executor
	pipelineMu        sync.RWMutex
}

// NewServer creates a new gRPC server instance.
//
// NewServerëŠ” ìƒˆë¡œìš´ gRPC ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
//
// Parameters:
//   - cfg: ì„œë²„ ì„¤ì •
//   - workerManager: Worker Pod ê´€ë¦¬ì
//   - k8sClient: Kubernetes API í´ë¼ì´ì–¸íŠ¸
//
// Returns:
//   - *Server: ì´ˆê¸°í™”ëœ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤
func NewServer(cfg *config.Config, workerManager *worker.Manager, k8sClient *k8s.Client) *Server {
	if cfg == nil || workerManager == nil || k8sClient == nil {
		panic("NewServer: nil parameters are not allowed")
	}

	log.Printf("ğŸš€ Ottoscaler gRPC ì„œë²„ ì´ˆê¸°í™” ì¤‘ (í¬íŠ¸: %d)", cfg.GRPC.Port)

	// Initialize log streaming server with Otto-handler configuration
	ottoHandlerAddress := cfg.GRPC.OttoHandlerHost
	mockMode := cfg.GRPC.MockMode

	if mockMode {
		log.Printf("ğŸ­ MOCK ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ - Otto-handler ë¡œê·¸ê°€ ì‹œë®¬ë ˆì´ì…˜ë©ë‹ˆë‹¤")
	} else {
		log.Printf("ğŸ”— Otto-handler ì—°ê²° ì¤‘: %s", ottoHandlerAddress)
	}

	logStreamServer := NewLogStreamingServer(k8sClient, ottoHandlerAddress, mockMode)

	return &Server{
		config:            cfg,
		workerManager:     workerManager,
		k8sClient:         k8sClient,
		logStreamServer:   logStreamServer,
		pipelineExecutors: make(map[string]*pipeline.Executor),
	}
}

// ScaleUp handles scale up requests from otto-handler.
//
// ScaleUpì€ otto-handlerë¡œë¶€í„° ìŠ¤ì¼€ì¼ ì—… ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
// ìš”ì²­ëœ ìˆ˜ë§Œí¼ Worker Podë¥¼ ìƒì„±í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
func (s *Server) ScaleUp(ctx context.Context, req *pb.ScaleRequest) (*pb.ScaleResponse, error) {
	if req == nil {
		return nil, status.Error(codes.InvalidArgument, "request cannot be nil")
	}

	startTime := time.Now()
	log.Printf("ğŸ“ˆ ScaleUp ìš”ì²­ ìˆ˜ì‹ : task_id=%s, worker_count=%d, repository=%s",
		req.TaskId, req.WorkerCount, req.Repository)

	// Validate request
	if req.TaskId == "" {
		return nil, status.Error(codes.InvalidArgument, "task_id is required")
	}
	if req.WorkerCount <= 0 {
		return nil, status.Error(codes.InvalidArgument, "worker_count must be positive")
	}

	// Create worker configurations
	workerConfigs := s.createWorkerConfigs(req)

	// Extract worker names for response
	workerPodNames := make([]string, len(workerConfigs))
	for i, config := range workerConfigs {
		workerPodNames[i] = config.Name
	}

	// Run workers using worker manager (in background to not block gRPC response)
	go func() {
		workerCtx := context.Background() // Use independent context for worker execution
		if err := s.workerManager.RunMultipleWorkers(workerCtx, workerConfigs); err != nil {
			log.Printf("âŒ íƒœìŠ¤í¬ %sì˜ Worker ì‹¤í–‰ ì˜¤ë¥˜: %v", req.TaskId, err)
		}
	}()

	response := &pb.ScaleResponse{
		Status:         pb.ScaleResponse_SUCCESS,
		Message:        fmt.Sprintf("Successfully started %d workers for task %s", req.WorkerCount, req.TaskId),
		ProcessedCount: req.WorkerCount,
		WorkerPodNames: workerPodNames,
		StartedAt:      startTime.Format(time.RFC3339),
		CompletedAt:    time.Now().Format(time.RFC3339),
	}

	log.Printf("âœ… ScaleUp ì™„ë£Œ: task_id=%s, ì²˜ë¦¬ëœ ìˆ˜=%d, ì†Œìš” ì‹œê°„=%v",
		req.TaskId, response.ProcessedCount, time.Since(startTime))

	return response, nil
}

// ScaleDown handles scale down requests from otto-handler.
//
// ScaleDownì€ otto-handlerë¡œë¶€í„° ìŠ¤ì¼€ì¼ ë‹¤ìš´ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
// ì§€ì •ëœ Worker Podë“¤ì„ ì¢…ë£Œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
func (s *Server) ScaleDown(ctx context.Context, req *pb.ScaleRequest) (*pb.ScaleResponse, error) {
	if req == nil {
		return nil, status.Error(codes.InvalidArgument, "request cannot be nil")
	}

	startTime := time.Now()
	log.Printf("ğŸ“‰ ScaleDown ìš”ì²­ ìˆ˜ì‹ : task_id=%s, ëª©í‘œ ìˆ˜=%d",
		req.TaskId, req.WorkerCount)

	// Validate request
	if req.TaskId == "" {
		return nil, status.Error(codes.InvalidArgument, "task_id is required")
	}

	// TODO: Implement actual worker termination logic
	response := &pb.ScaleResponse{
		Status:         pb.ScaleResponse_SUCCESS,
		Message:        fmt.Sprintf("Successfully processed scale down request for task %s", req.TaskId),
		ProcessedCount: 0,          // TODO: Fill with actual terminated count
		WorkerPodNames: []string{}, // TODO: Fill with actual terminated pod names
		StartedAt:      startTime.Format(time.RFC3339),
		CompletedAt:    time.Now().Format(time.RFC3339),
	}

	log.Printf("âœ… ScaleDown ì™„ë£Œ: task_id=%s, ì²˜ë¦¬ëœ ìˆ˜=%d, ì†Œìš” ì‹œê°„=%v",
		req.TaskId, response.ProcessedCount, time.Since(startTime))

	return response, nil
}

// GetWorkerStatus handles worker status requests from otto-handler.
//
// GetWorkerStatusëŠ” otto-handlerë¡œë¶€í„° Worker ìƒíƒœ ì¡°íšŒ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
// í˜„ì¬ í™œì„± ìƒíƒœì¸ Worker Podë“¤ì˜ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
func (s *Server) GetWorkerStatus(ctx context.Context, req *pb.WorkerStatusRequest) (*pb.WorkerStatusResponse, error) {
	if req == nil {
		return nil, status.Error(codes.InvalidArgument, "request cannot be nil")
	}

	log.Printf("ğŸ“Š GetWorkerStatus ìš”ì²­ ìˆ˜ì‹ : task_id=%s, ìƒíƒœ í•„í„°=%s",
		req.TaskId, req.StatusFilter)

	// Get worker statuses from Kubernetes
	workerStatuses, err := s.convertWorkerStatusesToPB(ctx, req.TaskId)
	if err != nil {
		log.Printf("âŒ Worker ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: %v", err)
		return nil, status.Errorf(codes.Internal, "failed to retrieve worker statuses: %v", err)
	}

	// Calculate status counts
	running, pending, succeeded, failed := calculateStatusCounts(workerStatuses)

	response := &pb.WorkerStatusResponse{
		TotalCount:     int32(len(workerStatuses)),
		RunningCount:   running,
		PendingCount:   pending,
		SucceededCount: succeeded,
		FailedCount:    failed,
		Workers:        workerStatuses,
	}

	log.Printf("âœ… GetWorkerStatus ì™„ë£Œ: ì´=%d, ì‹¤í–‰ ì¤‘=%d, ëŒ€ê¸° ì¤‘=%d",
		response.TotalCount, response.RunningCount, response.PendingCount)

	return response, nil
}

// ExecutePipeline handles pipeline execution requests from otto-handler.
//
// ExecutePipelineì€ otto-handlerë¡œë¶€í„° Pipeline ì‹¤í–‰ ìš”ì²­ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
// ì „ì²´ Pipelineì„ ë°›ì•„ Stageë³„ë¡œ ë¶„ì„í•˜ê³  ì˜ì¡´ì„±ì— ë”°ë¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
func (s *Server) ExecutePipeline(req *pb.PipelineRequest, stream pb.OttoscalerService_ExecutePipelineServer) error {
	if req == nil {
		return status.Error(codes.InvalidArgument, "request cannot be nil")
	}
	
	// Validate request
	if req.PipelineId == "" {
		return status.Error(codes.InvalidArgument, "pipeline_id is required")
	}
	if len(req.Stages) == 0 {
		return status.Error(codes.InvalidArgument, "at least one stage is required")
	}
	
	log.Printf("ğŸš€ ExecutePipeline ìš”ì²­ ìˆ˜ì‹ : pipeline_id=%s, name=%s, stages=%d",
		req.PipelineId, req.Name, len(req.Stages))
	
	// Check if pipeline is already running
	s.pipelineMu.RLock()
	if _, exists := s.pipelineExecutors[req.PipelineId]; exists {
		s.pipelineMu.RUnlock()
		return status.Error(codes.AlreadyExists, 
			fmt.Sprintf("pipeline %s is already running", req.PipelineId))
	}
	s.pipelineMu.RUnlock()
	
	// Create new executor
	executor := pipeline.NewExecutor(s.workerManager, s.config.Kubernetes.Namespace)
	
	// Store executor
	s.pipelineMu.Lock()
	s.pipelineExecutors[req.PipelineId] = executor
	s.pipelineMu.Unlock()
	
	// Cleanup executor when done
	defer func() {
		s.pipelineMu.Lock()
		delete(s.pipelineExecutors, req.PipelineId)
		s.pipelineMu.Unlock()
		log.Printf("ğŸ§¹ Pipeline executor ì •ë¦¬: %s", req.PipelineId)
	}()
	
	// Start pipeline execution
	ctx := stream.Context()
	progressChan, err := executor.Execute(ctx, req)
	if err != nil {
		log.Printf("âŒ Pipeline ì‹¤í–‰ ì‹œì‘ ì‹¤íŒ¨: %v", err)
		return status.Error(codes.Internal, 
			fmt.Sprintf("failed to start pipeline: %v", err))
	}
	
	// Stream progress updates
	for progress := range progressChan {
		if err := stream.Send(progress); err != nil {
			log.Printf("âŒ Progress ì „ì†¡ ì‹¤íŒ¨: %v", err)
			executor.Cancel()
			return err
		}
	}
	
	log.Printf("âœ… Pipeline ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: %s", req.PipelineId)
	return nil
}

// Start starts the gRPC server and begins listening for requests.
//
// StartëŠ” gRPC ì„œë²„ë¥¼ ì‹œì‘í•˜ê³  ìš”ì²­ ëŒ€ê¸°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
// Contextê°€ ì·¨ì†Œë  ë•Œê¹Œì§€ ë¸”ë¡œí‚¹ë©ë‹ˆë‹¤.
func (s *Server) Start(ctx context.Context) error {
	addr := s.config.GetGRPCAddr()

	listener, err := net.Listen("tcp", addr)
	if err != nil {
		return fmt.Errorf("failed to listen on %s: %w", addr, err)
	}

	grpcServer := grpc.NewServer()
	pb.RegisterOttoscalerServiceServer(grpcServer, s)
	pb.RegisterLogStreamingServiceServer(grpcServer, s.logStreamServer)

	log.Printf("ğŸ¯ gRPC ì„œë²„ ì‹œì‘ (ì£¼ì†Œ: %s)", addr)

	// Start periodic cleanup for log streaming server
	cleanupTicker := time.NewTicker(5 * time.Minute)
	defer cleanupTicker.Stop()

	go func() {
		for {
			select {
			case <-cleanupTicker.C:
				s.logStreamServer.CleanupInactiveSessions()
			case <-ctx.Done():
				return
			}
		}
	}()

	// Start server in goroutine
	errChan := make(chan error, 1)
	go func() {
		if err := grpcServer.Serve(listener); err != nil {
			errChan <- fmt.Errorf("gRPC server failed: %w", err)
		}
	}()

	// Wait for context cancellation or server error
	select {
	case <-ctx.Done():
		log.Println("ğŸ›‘ Shutting down gRPC server...")

		// Stop all active log collections
		log.Printf("ğŸ“œ í™œì„± ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° ì„¸ì…˜: %dê°œ", s.logStreamServer.GetActiveSessionsCount())

		grpcServer.GracefulStop()
		log.Println("âœ… gRPC server stopped gracefully")
		return ctx.Err()
	case err := <-errChan:
		return err
	}
}
